{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_dataset = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_dataset = pd.read_csv('/kaggle/input/titanic/test.csv')\ntrain_dataset['train_test'] = 1\ntest_dataset['train_test'] = 0\ntest_dataset['Survived'] = np.NaN\nall_data = pd.concat([train_dataset,test_dataset])\n\n%matplotlib inline\nall_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset.describe().columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separate numeric data & categorical\ndf_num = train_dataset[['Age','SibSp','Parch','Fare']]\ndf_cat = train_dataset[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot all numeric data\nfor i in df_num.columns:\n    plt.hist(df_num[i])\n    plt.title(i)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_num.corr())\nsns.heatmap(df_num.corr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare survived rate through numeric data\npd.pivot_table(train_dataset, index='Survived', values=['Age','SibSp','Parch','Fare'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare categorical data\nfor i in df_cat.columns:\n    sns.barplot(df_cat[i].value_counts().index,df_cat[i].value_counts()).set_title(i)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare survived by categorical data\nprint(pd.pivot_table(train_dataset, index = 'Survived', columns = 'Pclass', values = 'Ticket', aggfunc = 'count'))\nprint()\nprint(pd.pivot_table(train_dataset, index = 'Survived', columns = 'Sex', values = 'Ticket', aggfunc = 'count'))\nprint()\nprint(pd.pivot_table(train_dataset, index = 'Survived', columns = 'Embarked', values = 'Ticket', aggfunc = 'count'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# excute messy data Cabin & Ticket\ndf_cat.Cabin\ntrain_dataset['cabin_multiple'] = train_dataset.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n# after looking at this, we may want to look at cabin by letter or by number. Let's create some categories for this \n# letters \n# multiple letters \ntrain_dataset['cabin_multiple'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.pivot_table(train_dataset, index = 'Survived', columns = 'cabin_multiple', values = 'Name', aggfunc = 'count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creates categories based on the cabin letter (n stands for null)\n#in this case we will treat null values like it's own category\ntrain_dataset['cabin_adv'] = train_dataset.Cabin.apply(lambda x: str(x)[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare survival by cabin\nprint(train_dataset.cabin_adv.value_counts())\npd.pivot_table(train_dataset, index = 'Survived', columns = 'cabin_adv', values = 'Name', aggfunc = 'count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#understand ticket values better \n#numeric vs non numeric \ntrain_dataset['numeric_ticket'] = train_dataset.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntrain_dataset['ticket_letters'] = train_dataset.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) > 0 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset['ticket_letters'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset['numeric_ticket'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.pivot_table(train_dataset, index = 'Survived', columns = 'numeric_ticket', values = 'Ticket', aggfunc = 'count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.pivot_table(train_dataset, index = 'Survived', columns = 'ticket_letters', values = 'Ticket', aggfunc = 'count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature engineering person'titles\ntrain_dataset.Name.head(50)\ntrain_dataset['name_title'] = train_dataset.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n# mr., ms., a.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset['name_title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create all categorical data we did for both train & test\nall_data['cabin_multiple'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nall_data['cabin_adv'] = all_data.Cabin.apply(lambda x: str(x)[0])\nall_data['numeric_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nall_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) > 0 else 0)\nall_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n# hide null data\nall_data.Age = all_data.Age.fillna(train_dataset.Age.median())\nall_data.Fare = all_data.Fare.fillna(train_dataset.Fare.median())\n# delete missing data\nall_data.dropna(subset = ['Embarked'], inplace = True)\n# log norm data (not used)\nall_data['norm_sibsp'] = np.log(all_data.SibSp+1)\nall_data['norm_sibsp'].hist()\n# log norm fare (used)\nall_data['norm_fare'] = np.log(all_data.Fare+1)\nall_data['norm_fare'].hist()\n# convert fare to categorical data\nall_data.Pclass = all_data.Pclass.astype(str)\n# create fake variable (can use one-hot encoder)\nall_dumies = pd.get_dummies(all_data[['Pclass','Sex','Age','SibSp','Parch','norm_fare','Embarked','cabin_adv','cabin_multiple','numeric_ticket','name_title','train_test']])\n# split to train test again\nX_train = all_dumies[all_dumies.train_test == 1].drop(['train_test'], axis = 1)\nX_test = all_dumies[all_dumies.train_test == 0].drop(['train_test'], axis = 1)\ny_train = all_data[all_data.train_test == 1].Survived\ny_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale data\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nall_dumies_scaled = all_dumies.copy()\nall_dumies_scaled[['Age', 'SibSp', 'Parch', 'norm_fare']] = scale.fit_transform(all_dumies_scaled[['Age', 'SibSp', 'Parch', 'norm_fare']])\nall_dumies_scaled\nX_train_scaled = all_dumies_scaled[all_dumies_scaled.train_test == 1].drop(['train_test'], axis = 1)\nX_test_scaled = all_dumies_scaled[all_dumies_scaled.train_test == 0].drop(['train_test'], axis = 1)\ny_train = all_data[all_data.train_test == 1].Survived","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb = GaussianNB()\ncv = cross_val_score(gnb, X_train_scaled, y_train, cv = 5)\nprint(cv)\nprint(cv.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr, X_train_scaled, y_train, cv = 5)\nprint(cv)\nprint(cv.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt, X_train_scaled, y_train, cv = 5)\nprint(cv)\nprint(cv.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn, X_train_scaled, y_train, cv = 5)\nprint(cv)\nprint(cv.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf, X_train_scaled, y_train, cv = 5)\nprint(cv)\nprint(cv.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(probability = True)\ncv = cross_val_score(svc,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nxgb = XGBClassifier(random_state = 1)\ncv = cross_val_score(xgb,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# performance report\ndef clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score : ' + str(classifier.best_score_))\n    print('Best Parametters : ' + str(classifier.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nparam_grid = {'max_iter': [2000],\n              'penalty' : ['l1','l2'],\n              'C': np.logspace(-4,4,20),\n              'solver' : ['liblinear']}\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_lr = clf_lr.fit(X_train_scaled, y_train)\nclf_performance(best_clf_lr, 'Logistic Regression')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors' : [3,5,7,9],\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]}\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(X_train_scaled,y_train)\nclf_performance(best_clf_knn,'KNN')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train_scaled,y_train)\nclf_performance(best_clf_svc,'SVC')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Because the total feature space is so large, I used a randomized search to narrow down the paramters for the model. I took the best model from this and did a more granular search \n\"\"\"\nrf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [100,500,1000], \n                                  'bootstrap': [True,False],\n                                  'max_depth': [3,5,10,20,50,75,100,None],\n                                  'max_features': ['auto','sqrt'],\n                                  'min_samples_leaf': [1,2,4,10],\n                                  'min_samples_split': [2,5,10]}\n                                  \nclf_rf_rnd = RandomizedSearchCV(rf, param_distributions = param_grid, n_iter = 100, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf_rnd = clf_rf_rnd.fit(X_train_scaled,y_train)\nclf_performance(best_clf_rf_rnd,'Random Forest')\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [400,450,500,550],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [15, 20, 25],\n                                  'max_features': ['auto','sqrt', 10],\n                                  'min_samples_leaf': [2,3],\n                                  'min_samples_split': [2,3]}\nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True)\nbest_clf_rf = clf_rf.fit(X_train_scaled, y_train)\nclf_performance(best_clf_rf, 'Random Forest Classification')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_rf = best_clf_rf.best_estimator_.fit(X_train_scaled, y_train)\nfeat_importances = pd.Series(best_rf.feature_importances_, index = X_train_scaled.columns)\nfeat_importances.nlargest(20).plot(kind = 'barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"xgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [20, 50, 100, 250, 500,1000],\n    'colsample_bytree': [0.2, 0.5, 0.7, 0.8, 1],\n    'max_depth': [2, 5, 10, 15, 20, 25, None],\n    'reg_alpha': [0, 0.5, 1],\n    'reg_lambda': [1, 1.5, 2],\n    'subsample': [0.5,0.6,0.7, 0.8, 0.9],\n    'learning_rate':[.01,0.1,0.2,0.3,0.5, 0.7, 0.9],\n    'gamma':[0,.01,.1,1,10,100],\n    'min_child_weight':[0,.01,0.1,1,10,100],\n    'sampling_method': ['uniform', 'gradient_based']\n}\n\n#clf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n#best_clf_xgb = clf_xgb.fit(X_train_scaled,y_train)\n#clf_performance(best_clf_xgb,'XGB')\nclf_xgb_rnd = RandomizedSearchCV(xgb, param_distributions = param_grid, n_iter = 1000, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb_rnd = clf_xgb_rnd.fit(X_train_scaled,y_train)\nclf_performance(best_clf_xgb_rnd,'XGB')\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [450,500,550],\n    'colsample_bytree': [0.75,0.8,0.85],\n    'max_depth': [None],\n    'reg_alpha': [1],\n    'reg_lambda': [2, 5, 10],\n    'subsample': [0.55, 0.6, .65],\n    'learning_rate':[0.5],\n    'gamma':[.5,1,2],\n    'min_child_weight':[0.01],\n    'sampling_method': ['uniform']\n}\n\nclf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb = clf_xgb.fit(X_train_scaled,y_train)\nclf_performance(best_clf_xgb,'XGB')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_xgb = best_clf_xgb.best_estimator_.predict(X_test_scaled).astype(int)\nxgb_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_xgb}\nsubmission_xgb = pd.DataFrame(data=xgb_submission)\nsubmission_xgb.to_csv('xgb_submission3.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_lr = best_clf_lr.best_estimator_\nbest_knn = best_clf_knn.best_estimator_\nbest_svc = best_clf_svc.best_estimator_\nbest_rf = best_clf_rf.best_estimator_\nbest_xgb = best_clf_xgb.best_estimator_\n\nvoting_clf_hard = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting = 'hard') \nvoting_clf_soft = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting = 'soft') \nvoting_clf_all = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc), ('lr', best_lr)], voting = 'soft') \nvoting_clf_xgb = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc), ('xgb', best_xgb),('lr', best_lr)], voting = 'soft')\n\nprint('voting_clf_hard :',cross_val_score(voting_clf_hard,X_train,y_train,cv=5))\nprint('voting_clf_hard mean :',cross_val_score(voting_clf_hard,X_train,y_train,cv=5).mean())\n\nprint('voting_clf_soft :',cross_val_score(voting_clf_soft,X_train,y_train,cv=5))\nprint('voting_clf_soft mean :',cross_val_score(voting_clf_soft,X_train,y_train,cv=5).mean())\n\nprint('voting_clf_all :',cross_val_score(voting_clf_all,X_train,y_train,cv=5))\nprint('voting_clf_all mean :',cross_val_score(voting_clf_all,X_train,y_train,cv=5).mean())\n\nprint('voting_clf_xgb :',cross_val_score(voting_clf_xgb,X_train,y_train,cv=5))\nprint('voting_clf_xgb mean :',cross_val_score(voting_clf_xgb,X_train,y_train,cv=5).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'weights' : [[1,1,1],[1,2,1],[1,1,2],[2,1,1],[2,2,1],[1,2,2],[2,1,2]]}\n\nvote_weight = GridSearchCV(voting_clf_soft, param_grid = params, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_weight = vote_weight.fit(X_train_scaled,y_train)\nclf_performance(best_clf_weight,'VC Weights')\nvoting_clf_sub = best_clf_weight.best_estimator_.predict(X_test_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make Predictions \nvoting_clf_hard.fit(X_train_scaled, y_train)\nvoting_clf_soft.fit(X_train_scaled, y_train)\nvoting_clf_all.fit(X_train_scaled, y_train)\nvoting_clf_xgb.fit(X_train_scaled, y_train)\n\nbest_rf.fit(X_train_scaled, y_train)\ny_hat_vc_hard = voting_clf_hard.predict(X_test_scaled).astype(int)\ny_hat_rf = best_rf.predict(X_test_scaled).astype(int)\ny_hat_vc_soft =  voting_clf_soft.predict(X_test_scaled).astype(int)\ny_hat_vc_all = voting_clf_all.predict(X_test_scaled).astype(int)\ny_hat_vc_xgb = voting_clf_xgb.predict(X_test_scaled).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert output to dataframe \nfinal_data = {'PassengerId': test.PassengerId, 'Survived': y_hat_rf}\nsubmission = pd.DataFrame(data=final_data)\n\nfinal_data_2 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_hard}\nsubmission_2 = pd.DataFrame(data=final_data_2)\n\nfinal_data_3 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_soft}\nsubmission_3 = pd.DataFrame(data=final_data_3)\n\nfinal_data_4 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_all}\nsubmission_4 = pd.DataFrame(data=final_data_4)\n\nfinal_data_5 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_xgb}\nsubmission_5 = pd.DataFrame(data=final_data_5)\n\nfinal_data_comp = {'PassengerId': test.PassengerId, 'Survived_vc_hard': y_hat_vc_hard, 'Survived_rf': y_hat_rf, 'Survived_vc_soft' : y_hat_vc_soft, 'Survived_vc_all' : y_hat_vc_all,  'Survived_vc_xgb' : y_hat_vc_xgb}\ncomparison = pd.DataFrame(data=final_data_comp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#track differences between outputs \ncomparison['difference_rf_vc_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_rf else 0, axis =1)\ncomparison['difference_soft_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_vc_soft else 0, axis =1)\ncomparison['difference_hard_all'] = comparison.apply(lambda x: 1 if x.Survived_vc_all != x.Survived_vc_hard else 0, axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comparison.difference_hard_all.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare submission files \nsubmission.to_csv('submission_rf.csv', index =False)\nsubmission_2.to_csv('submission_vc_hard.csv',index=False)\nsubmission_3.to_csv('submission_vc_soft.csv', index=False)\nsubmission_4.to_csv('submission_vc_all.csv', index=False)\nsubmission_5.to_csv('submission_vc_xgb2.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}